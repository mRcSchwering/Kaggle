---
title: "Keine Panik auf der Titanic"
author: "Marc Schwering"
output:
  html_document:
    number_sections: true
    toc: true
    toc_depth: 2
    fig_width: 7
    fig_height: 4.5
    theme: cosmo
---







# Introduction

This is my first Kaggle script so don't go too hard on me.
I used some machine learning in the past but only to solve biological problems,
so this is new.

While analyzing the dataset I will go through **Data Preparation**, 
**Modeling**, and **Prediction**.
*Data Preparation* means I look at the data quality, extract features,
explore these features, and handle missing values.
During *Modeling* the data is shaped for the model class, a form of feature 
selection is performed, and models are fit.
Then the models are analyzed.
Finally, the best model is used to make a *Prediction* for the test dataset.

This sounds straight forward but usually it is not.
It might happen that I go back and forth bewteen consecutive steps.

**Linear Discriminant Analysis**

I have looked into some scripts for the Titanic set.
A lot of people use random forest predictors.
I have seen nobody using linear discriminant analysis (LDA) yet which would be
my first thought.
In the modelling part I will try out a LDA and some of its cousins.

**Loading Data**

```{r, message=FALSE, warning=FALSE, results='hide'}
library(data.table) # database-like tables
library(ggplot2) # general plots
library(mice) # for imputation
library(crossval) # for cross validation
library(MASS) # for LDA and QDA
library(sda) # LDA with special shrinkage method
```

Let's load the data.
As you can see I belong to the `data.table` people.

```{r}
test <- read.csv("../input/test.csv")
train <- read.csv("../input/train.csv")
test <- data.table(test)
train <- data.table(train)
str(train)
str(test)
```

Some classes should be adjusted.

```{r, message=FALSE, warning=FALSE, results='hide'}
train[, Name := as.character(Name)]
train[, Ticket := as.character(Ticket)]
train[, Cabin := as.character(Cabin)]
train[, Survived := as.factor(Survived)]
train[, Pclass := as.factor(Pclass)]

test[, Name := as.character(Name)]
test[, Ticket := as.character(Ticket)]
test[, Cabin := as.character(Cabin)]
test[, Pclass := as.factor(Pclass)]
```








***







# Data Preparation

## Data Quality

Here, I basically go through the data and see if there is something
important like missing values or weird values.
I don't really look at distributions or correlations yet.
This is done in a later section.
To keep it short I will only show the stuff that is interesting in some way.
For a lot of these features I just did `table` or `summary` to confirm that
they look fine.

### Age

This is the first feature where I found missing values.

```{r}
summary(train$Age)
summary(test$Age)
```

I guess this is an impotant feature, so probably we have to infer
something here.
The age range seems reasonable:
from 2 months to 80 years old.


### Fare

```{r}
summary(train$Fare)
summary(test$Fare)
```

There is one $NA$ *Fare* in the test set.

```{r}
test[is.na(Fare), ]
```

Also, in both *test* and *train* there are some wildly high values of 512.3.
What's up with these?

```{r}
train[Fare > 512, ]
test[Fare > 512, ]
```

And there were free rides:

```{r}
train[Fare == 0, ]
test[Fare == 0, ]
```

The cases with `Fare == 0` also have a lot of missing values for *Age*.
I wonder whether `0` also denotes to a missing value for *Fare*.

### Cabin

```{r}
head(train$Cabin)
head(test$Cabin)
table(train$Cabin == "")
table(test$Cabin == "")
```

This looks bad.
Most are empty.

### Embarked

```{r}
table(train$Embarked)
table(test$Embarked)
train[Embarked == "", ]
```

In the training set there are 2 empty *Embarked* entries.
Both survived, maybe they didn't embark.

### Summary

+ *Age*: many $NA$ in test and train
+ *Fare*: 1 $NA$ in test, several 0's
+ *Cabin*: many empty strings in test and train
+ *Embarked*: 2 empty strings in train








## Feature Extraction

In general one could use some of these features already as they are.
Now I see if I can extract some more features.
This is quite exploratory.
I go through the training set and see what I can do.

### Names

I am starting with the *Names* column.
Obviously one can extract *Surnames* and *Titles* from the names.
The idea for the following code is taken from `mrisdal`'s Titanic kernel.
Titles and surnames are extracted from the *Names* column.
To reduce the number of titles a label *Rare Title* is created. 
It marks all titles that did not appear that often.

**Titles**

```{r}
train$Title <- gsub('(.*, )|(\\..*)', '', train$Name)
table(train$Title)
```

```{r, message=FALSE, results='hide'}
rare_title <- c('Dona', 'Lady', 'the Countess','Capt', 'Col', 'Don', 
                'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer')
train[Title == "Mlle", Title := "Miss"]
train[Title == "Ms", Title := "Miss"]
train[Title == "Mme", Title := "Mrs"]
train[Title %in% rare_title, Title := "Rare Title"]
```

```{r}
table(train$Title, train$Sex)
```


**Surnames**

The idea for this part is also form `mrisdal`.

```{r}
train$Surname <- vapply(
  train$Name, 
  function(x) strsplit(x, "[,. ]")[[1]][1],
  character(1)
)
length(unique(train$Surname))
any(train$Surname == "")
```

**Surname Count**

I thought it would also be useful to have the number of people with identical
surnames aboard as a feature.
This might help later on when identifying families.

```{r}
counts <- table(train$Surname)
train$SNCount <- vapply(
  train$Surname, 
  function(sn) counts[names(counts) == sn],
  integer(1)
)
table(train$SNCount)
```

### Age

One important factor for survival is definitely the age.
As far as I know women and children were the first to board the rescue boats.
So, there should be a natural separation in the *Age* feature at 18.
I will extract this as a binary feature.

```{r}
train$isChild <- as.integer(train$Age < 18)
table(train$isChild)
```

### Families

Families probably stayed together.
So knowning the families would be good for prediction.
Using cabin information, Sibling/Spouses and Parent/Children, and the names
one could try to link family members.
For now I will just extract the general family size.

```{r}
train$FMembers <- train$SibSp + train$Parch + 1
table(train$FMembers)
```

### Ticket Text

The tickets are not only numbers.

```{r}
txt <- toupper(gsub("[0-9\\. /]", "", train$Ticket))
table(txt)
```

This seems to tell me the embarkment as well.
But this factor has to many levels in my opinion.
I will keep the most abundant levels and try to allocate the others
by similarity (that's quite subjective).
For the ones I don't know how to allocate I will create a label *other*.

```{r}
table(txt)[table(txt) > 10]
txt[txt == "AS"] <- "A"
txt[txt %in% c("C", "CASOTON")] <- "CA"
txt[txt %in% c("PC", "PPP", "PP")] <- "PC"
txt[txt %in% c("SC", "SCA")] <- "SCPARIS"
txt[txt %in% c("SOTONOQ", "STONO")] <- "SOTONO"
txt[!(txt %in% c("", "SOTONO", "SCPARIS", "PC", "CA"))] <- "other"
train$TicketText <- txt
table(train$TicketText)
```


### Cabins

The Cabin feature is very sparse.
However, for those that have values, we can extract the deck symbol and the 
number of cabins listed on the ticket.

**Deck** 

The highest deck should be *A*, the one below *B*, and so on.
The first character in the *Cabin* number is the deck.

```{r, results='hide'}
train$Deck <- vapply(
  train$Cabin, 
  function(x) strsplit(x, "")[[1]][1],
  character(1)
)
train[is.na(Deck), Deck := ""]
```

```{r}
table(train$Deck)
```

**Number of Cabins**

In some cases several cabins are listed.
Maybe that will be important.

```{r}
train$nCabins <- vapply(
  train$Cabin, 
  function(x) length(strsplit(x, " ")[[1]]),
  integer(1)
)
table(train$nCabins)
```

For the people who do not have a cabin I will also create a binary feature.

```{r}
train$hasCabin <- train$nCabins > 0
```








## Feature Exploration

In this section I will look at feature distributions and correlations.
This information is useful when constructing the final features for the model.

As said before I want to use linear discriminant analyses and related 
models.
It is known how these behave to certain data structures.
Highly correlated features for example can produce unstable coefficient
estimates.
Or differing covariance matrices among target classes (i.e. *survived*, 
*not survived*) violate the shared covariance assumption of the regular
LDA.
In general I will explore different combination of features in the **Modeling**
section anyway. 
But it is good to know the features in order to explain why certain model 
perform better than others.

Another thing is, that I might need to adjust some features.
A regular LDA for example assumes Gauss distributed features.
So, sometimes it makes sense to use transformations of a feature instead of
the raw feature (i.e. *basis expansion*).

Again, this is not a straight forward process.
I might extract some more features along the way.
Usually, I would go through each feature and look for correlation to 
(almost) every other feature.
To keep it short I will only print the inetresting plots and give a summary at
the end.


### Numeric Features Overview

At first I like to look at all numeric features, their correlation among each
others and their influence on the target variable.
A scatter plot matrix (SPLOM) is good for that.
In the following scatter plot matrix blue indicates *survived*.

```{r, fig.height = 10, fig.width = 10}
# color palette for Survived label
pal <- c("red", "blue")

# pearson corelation coefficients for SPLOM 
panel.cor <- function(x, y, digits = 2, cex.cor, ...) {
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(0, 1, 0, 1))
  m <- cbind(x, y)
  m <- m[complete.cases(m), ]
  r <- cor(m[, 1], m[, 2])
  txt <- format(c(r, 0.123456789), digits = digits)[1]
  txt <- paste("r= ", txt, sep = "")
  text(0.5, 0.5, txt)
}

# SPLOM
pairs(
  train[, .(Age, Fare, SibSp, Parch, FMembers, SNCount, nCabins)], 
  col = pal[train$Survived], 
  pch = 20,
  cex = 0.5,
  lower.panel = panel.cor
)
```

For discrete values one should be careful with the interpretation of these 
scatter plots. These few points that can be seen in distinct places are in 
reality many points plotted on top of each other.
The pearson correlation coefficients are still useful though.

For obvious reasons there is a high correlation between *SibSp* and *FMembers*, 
and *Parch* and *FMembers* (created from each other).
There is also a high correlation between *FMembers* and *SNCount*.
The correlation of *SNCount* to either *SibSp* or *Parch* is moderately high.

The remaining feature pairs do not show high correlations.

### Age

Let's see if the age has a substantial effect on survival chances.

I will plot densities for such comparisons.
They integrate to 1 which means I can nicely compare distributions of
2 features with different total counts.
Second, they use a kernel for local smoothing.
This way I only see the local trend.

```{r, warning=FALSE}
ggplot(train, aes(x = Age, fill = Survived)) +
  geom_density(alpha = .5) +
  theme_classic()
```

For both survivors and non-survivors the age distribution looks very similar.
There seems to be just one clear trend change between age 0 and 20.
This is probably because children boarded first.

However, from this plot I can't say whether this separation is really at the 
age of 18.
I can look at the proportions of survivors and non-survivors in 2 age groups
when setting the separation at different ages.

```{r}
dt <- train[!is.na(train$Age), 
            .(isU12 = Age < 12, isU14 = Age < 14,
              isU16 = Age < 16, isU18 = Age < 18,
              isU20 = Age < 20, isU22 = Age < 22,
              Survived)]
dt <- dt[, 
   .(isU12 = sum(isU12) / length(isU12), isU14 = sum(isU14) / length(isU14),
     isU16 = sum(isU16) / length(isU16), isU18 = sum(isU18) / length(isU18),
     isU20 = sum(isU20) / length(isU20), isU22 = sum(isU22) / length(isU22)), 
   by = Survived]
dt <- melt(dt, measure.vars = 2:7, variable.name = "Distinction",
           value.name = "People")
f <- function(x) { x[2] / x[1] }
dt <- dt[, .(Proportion = f(People)), by = Distinction]
ggplot(dt, aes(x = Distinction, y = Proportion, fill = Proportion)) +
  geom_bar(position = "dodge", stat = "identity") + 
  theme_classic()
```

According to this plot, 18 would not be a good age to group children and adults.
I should rather use age 14.

```{r}
train$isU14 <- train$Age < 14
mosaicplot(table(train[, .(isU14, Survived)]), shade = TRUE)
```

The mosaic plot visualizes the contingency table of 2 factors as rectangles.
The color here shows the residuals of a model which assumes independence of both
factors.
So, the more colorful the rectangles, the less they can be explained by a 
independence model.
Here, this means feature *isU14* seems to explain the variability in *Survived*
to some degree.

### Families

**SibSp**

To visualize the distribution of discrete features I will use bar plots.

```{r}
ggplot(train, aes(x = SibSp, fill = Survived)) +
  geom_bar(position = "dodge") + 
  theme_classic()
```

Both classes (*survivor*/ *non-survivor*) show similar trends but the 
proportions seem to differ.
This is easier to see if I use a mosaic plot.

```{r}
mosaicplot(table(train[, .(SibSp, Survived)]), shade = TRUE)
```

It seems that large families in general had lower survival chances.
Passangers with 1 spouse/sibling had the highest survival chances.
But then people with zero spouses/sibling had lower survival chances 
again.

Maybe families in general had higher survival chances because there were 
children involved.
But then for the larger families it was harder to get every one together in the
chaos and that's why they again have lower survival chances.
(My theory for this...)

A linear separator would not be able to distinguish drop at 0 again.
I could engineer a metric like this:

```{r}
f <- function(x) { (1 - x)^2 }
mosaicplot(table(f(train$SibSp), train$Survived), shade = TRUE)
```

However, I have weirdly high values now.
Knowing that this affects the LDA fit a lot, I rather create a binary feature
to only capture $SibSp = 1$.

```{r}
train$SibSpIs1 <- train$SibSp == 1
mosaicplot(table(train[, .(SibSpIs1, Survived)]), shade = TRUE)
```


**Parch, FMembers, SNCount**

To make it short: I have seen similar trends in the family related features.
I always extracted the cases with highest survival rate as a binary feature.

```{r}
train$ParchIs1 <- train$Parch == 1
train$smallFamily <- train$FMembers > 1 & train$FMembers < 5
train$SNCountIs2 <- train$SNCount == 2
```

### Fare

This feature can be spread out better by a log transformation.
The log10 seems natural for money.

```{r, warning=FALSE}
ggplot(train, aes(x = Fare, fill = Survived)) +
  geom_density(alpha = .5) +
  scale_x_continuous(trans = "log10") +
  theme_classic()
```

It looks like survival chances were higher if you had an expensive ticket.
The capitalism is strong with this one.

```{r}
train$lFare <- log10(train$Fare + 1)
```


### Pclass

```{r}
mosaicplot(table(train[, .(Pclass, Survived)]), shade = TRUE)
```

This feature looks useful.
Higher classes had better survial chances.

### Ticket Text

```{r, fig.width=10}
mosaicplot(table(train[, .(TicketText, Survived)]), shade = TRUE)
```

The *PC* label seems to be a useful information.
I will make a feature for this.

```{r}
train$isPC <- train$TicketText == "PC"
```


### Deck

```{r, fig.width=10}
mosaicplot(table(train[, .(Deck, Survived)]), shade = TRUE)
```

Having no deck seems to be an indicator for low survival chances.

```{r}
train$noDeck <- train$Deck == ""
```

### Cabins

I did not see a trend in the number of cabins, only that having no cabin
is a bad omen.

```{r}
mosaicplot(table(train[, .(Survived, hasCabin)]), shade = TRUE)
```

### Embarked

Interestingly, people embarking from Cherbourg had higher survival chances.

```{r}
mosaicplot(table(train[, .(Embarked, Survived)]), shade = TRUE)
train$fromC <- train$Embarked == "C"
```

### Sex

I think this one is obvious.

```{r}
mosaicplot(table(train[, .(Sex, Survived)]), shade = TRUE)
```

### Title

```{r, fig.height=8, fig.width=8}
mosaicplot(table(train[, .(Title, Survived)]), shade = TRUE)
```

Well, needless to say the female titles had higher survival chances.
The interesting thing here is that *Master* and *Rare Title* have different 
survival chances than *Mr*.

```{r}
mosaicplot(table(train[Title %in% c("Rare Title", "Mr", "Master"), 
                   .(Title, Survived)]), shade = TRUE)
train$isMaster <- train$Title == "Master"
```


### Summary

**Among Features**

+ *FMembers* is highly correlated to *SibSp*, *Parch*, and *SNCount*
+ *SNCount* is moderately correlated to *SibSp* and *Parch*
+ *SibSp* and *Parch* is barely correlated
+ binary *Family* features are correlated among each other
+ *Fare* correlates with *Pclass*
+ *Embarked* is correlated with *TicketText*
+ *isPC* correlates with *Fare*
+ *nCabins* is highly correlated with *Deck*
+ *noDeck* correlates with *Fare*
+ *fromC* is correlated to *hasCabin*
+ *Title* is correlated to *Sex*

**To Target**

+ *Age*: for $Age \geq 14$ similar distributions among classes; for $Age \lt 14$
  more survivors
+ Families as represented by *SibSp*, *Parch*, *FMembers*, *SNCount*: large
  families have lower survival chances
+ *Fare*: higher fares have higher survival chances
+ *Pclass*: higher passanger classes have higher survival chances
+ *isPC*: $TicketText = PC$ means higher survival chances
+ *hasCabin*: having no cabin means lower survival chances
+ *fromC*: people from Cherbourg had higher survival chances
+ *Sex*: females had higher survival chances
+ *isMaster*: higher survival chance with title *Master*




***





## Missing Values

+ *Fare*: 1 $NA$ in test, several 0's
median for $NA$

+ *Cabin*: many empty strings in test and train
keep, empty strings

+ *Embarked*: 2 empty strings in train
impute by price

```{r}
train[Embarked == "", Fare]
ggplot(train[!Embarked == ""], aes(x = Embarked, y = Fare)) +
  geom_violin(fill = "gray") +
  geom_boxplot(alpha = .5, varwidth = TRUE) +
  geom_hline(yintercept = 80, color = "red", linetype = 2) +
  scale_y_continuous(trans = "log10") +
  theme_classic()

```



+ *Age*: many $NA$ in test and train

```{r}
imp <- mice(train[, .(Age, Pclass, Sex, lFare, Parch, SibSp, nCabins)], 
            seed = 42)
imp
xyplot(imp, Age ~ lFare | .imp)

head(complete(imp))
```


