---
title: "Keine Panik auf der Titanic"
author: "Marc Schwering"
output:
  html_document:
    number_sections: true
    toc: true
    toc_depth: 2
    fig_width: 7
    fig_height: 4.5
    theme: cosmo
---







# Introduction

This is my first Kaggle script so if you see any mistakes, please comment.
I used some machine learning in the past but only to solve biological problems,
so this is new.

While analyzing the dataset I will go through **Data Preparation**, 
**Modeling**, and **Prediction**.
*Data Preparation* means I look at the data quality, extract features,
explore these features, and handle missing values.
During *Modeling* the data is shaped for the model class, a form of feature 
selection is performed, and models are fit.
Then the models are analyzed.
Finally, the best model is used to make a *Prediction* for the test dataset.

This sounds straight forward but usually it is not.
It might happen that I go back and forth bewteen consecutive steps.

**Linear Discriminant Analysis**

Many kernels use logistic regression or random forests as model class.
I have not seen someone try out a linear discriminant analysis (LDA) yet.
Therefore, this kernel will try to make a prediction using LDA 
(and its relatives).

**Loading Data**

```{r, message=FALSE, warning=FALSE, results='hide'}
library(data.table) # database-like tables
library(ggplot2) # general plots
library(lattice) # also for plots
library(mice) # for imputation
library(klaR) # general functions for classification
library(sda) # LDA with special shrinkage method
library(crossval) # for cross validation
```

Let's load the data.
As you can see I belong to the `data.table` people.

```{r}
test <- read.csv("../input/test.csv")
train <- read.csv("../input/train.csv")
test <- data.table(test)
train <- data.table(train)
str(train)
str(test)
```

Some classes should be adjusted.
(This is the only time I am talking about the data structure.
From now on *class* means *survivor* or *non-survivor*)

```{r, message=FALSE, warning=FALSE, results='hide'}
train[, Name := as.character(Name)]
train[, Ticket := as.character(Ticket)]
train[, Cabin := as.character(Cabin)]
train[, Survived := as.factor(Survived)]
train[, Pclass := as.factor(Pclass)]

test[, Name := as.character(Name)]
test[, Ticket := as.character(Ticket)]
test[, Cabin := as.character(Cabin)]
test[, Pclass := as.factor(Pclass)]
```








***







# Data Preparation

## Data Quality

Here, I basically go through the data and see if there is something
important like missing values or weird values.
I don't really look at distributions or correlations yet.
This is done in a later section.
To keep it short I will only show the stuff that is interesting in some way.
For a lot of these features I just did `table` or `summary` to confirm that
they look fine.

### Age

This is the first feature where I found missing values.

```{r}
summary(train$Age)
summary(test$Age)
```

I guess this is an impotant feature, so probably we have to infer
something here.
The age range seems reasonable:
from 2 months to 80 years old.


### Fare

```{r}
summary(train$Fare)
summary(test$Fare)
```

There is one $NA$ *Fare* in the test set.

```{r}
test[is.na(Fare), ]
```

Also, in both *test* and *train* there are some wildly high values of 512.3.
What's up with these?

```{r}
train[Fare > 512, ]
test[Fare > 512, ]
```

And there were free rides:

```{r}
train[Fare == 0, ]
test[Fare == 0, ]
```

The cases with $Fare = 0$ also have a lot of missing values for *Age*.
I wonder whether `0` also denotes to a missing value for *Fare*.

### Cabin

```{r}
head(train$Cabin)
head(test$Cabin)
table(train$Cabin == "")
table(test$Cabin == "")
```

This looks bad.
Most are empty.

### Embarked

```{r}
table(train$Embarked)
train[Embarked == "", ]
```

In the training set there are 2 empty *Embarked* entries.
Both survived, maybe they didn't embark.

### Summary

+ *Age*: many $NA$ in test and train
+ *Fare*: 1 $NA$ in test, several 0's
+ *Cabin*: many empty strings in test and train
+ *Embarked*: 2 empty strings in train








## Feature Extraction

In general one could use some of these features already as they are.
Now I see if I can extract some more features.
This is quite exploratory.
I go through the training set and see what I can do.

### Names

I am starting with the *Names* column.
Obviously one can extract *Surnames* and *Titles* from the names.
The idea for the following code is taken from `mrisdal`'s Titanic kernel.
Titles and surnames are extracted from the *Names* column.
To reduce the number of titles a label *Rare Title* is created. 
It marks all titles that did not appear that often.

**Titles**

```{r}
train$Title <- gsub('(.*, )|(\\..*)', '', train$Name)
table(train$Title)
```

```{r, message=FALSE, results='hide'}
rare_title <- c('Dona', 'Lady', 'the Countess','Capt', 'Col', 'Don', 
                'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer')
train[Title == "Mlle", Title := "Miss"]
train[Title == "Ms", Title := "Miss"]
train[Title == "Mme", Title := "Mrs"]
train[Title %in% rare_title, Title := "Rare Title"]
```

```{r}
table(train$Title, train$Sex)
```


**Surnames**

The idea for this part is also form `mrisdal`.

```{r}
train$Surname <- vapply(
  train$Name, 
  function(x) strsplit(x, "[,. ]")[[1]][1],
  character(1)
)
length(unique(train$Surname))
any(train$Surname == "")
```

**Surname Count**

I thought it would also be useful to have the number of people with identical
surnames aboard as a feature.
This might help later on when identifying families.

```{r}
counts <- table(train$Surname)
train$SNCount <- vapply(
  train$Surname, 
  function(sn) counts[names(counts) == sn],
  integer(1)
)
table(train$SNCount)
```

### Age

One important factor for survival is definitely the age.
As far as I know women and children were the first to board the rescue boats.
So, there should be a natural separation at age of 18.
I will extract this as a binary feature.

```{r}
train$isChild <- as.integer(train$Age < 18)
table(train$isChild)
```

### Families

Families probably stayed together.
So knowning the families would be good for prediction.
Using cabin information, Sibling/Spouses and Parent/Children, and the names
one could try to link family members.
For now I will just extract the general family size.

```{r}
train$FMembers <- train$SibSp + train$Parch + 1
table(train$FMembers)
```

### Ticket Text

The tickets are not only numbers.

```{r}
txt <- toupper(gsub("[0-9\\. /]", "", train$Ticket))
table(txt)
```

This seems to tell me the embarkment as well.
But this factor has to many levels in my opinion.
I will keep the most abundant levels and try to allocate the others
by similarity (that's quite subjective).
For the ones I don't know how to allocate I will create a label *other*.

```{r}
table(txt)[table(txt) > 10]
txt[txt == "AS"] <- "A"
txt[txt %in% c("C", "CASOTON")] <- "CA"
txt[txt %in% c("PC", "PPP", "PP")] <- "PC"
txt[txt %in% c("SC", "SCA")] <- "SCPARIS"
txt[txt %in% c("SOTONOQ", "STONO")] <- "SOTONO"
txt[!(txt %in% c("", "SOTONO", "SCPARIS", "PC", "CA"))] <- "other"
train$TicketText <- txt
table(train$TicketText)
```


### Cabins

The Cabin feature is very sparse.
However, for those that have values, we can extract the deck symbol and the 
number of cabins listed on the ticket.

**Deck** 

The highest deck should be *A*, the one below *B*, and so on.
The first character in the *Cabin* number is the deck.

```{r, results='hide'}
train$Deck <- vapply(
  train$Cabin, 
  function(x) strsplit(x, "")[[1]][1],
  character(1)
)
train[is.na(Deck), Deck := ""]
```

```{r}
table(train$Deck)
```

**Number of Cabins**

In some cases several cabins are listed.
Maybe that will be important.

```{r}
train$nCabins <- vapply(
  train$Cabin, 
  function(x) length(strsplit(x, " ")[[1]]),
  integer(1)
)
table(train$nCabins)
```

For the people who do not have a cabin I will also create a binary feature.

```{r}
train$hasCabin <- train$nCabins > 0
```








## Feature Exploration

In this section I will look at feature distributions and correlations.
This information is useful when constructing the final features for the model.

As said before I want to use linear discriminant analyses and related 
models.
It is known how these behave to certain data structures.
Highly correlated features for example can produce unstable coefficient
estimates.
In general I will explore different combination of features in the **Modeling**
section anyway. 
But it is good to know the features in order to explain why certain models 
perform better than others.

Another thing is, that I might need to adjust some features.
A regular LDA for example assumes Gauss distributed features.
So, sometimes it makes sense to use transformations of a feature instead of
the raw feature.

Again, this is not a straight forward process.
I might extract some more features along the way.
Usually, I would go through each feature and look for correlation to 
(almost) every other feature.
To keep it short I will only print the inetresting plots and give a summary at
the end.


### Numeric Features Overview

At first I like to look at all numeric features, their correlation among each
others and their influence on the target variable.
A scatter plot matrix (SPLOM) is good for that.
In the following scatter plot matrix blue indicates *survived*.

```{r, fig.height = 10, fig.width = 10}
# color palette for Survived label
pal <- c("red", "blue")

# pearson corelation coefficients for SPLOM 
panel.cor <- function(x, y, digits = 2, cex.cor, ...) {
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(0, 1, 0, 1))
  m <- cbind(x, y)
  m <- m[complete.cases(m), ]
  r <- cor(m[, 1], m[, 2])
  txt <- format(c(r, 0.123456789), digits = digits)[1]
  txt <- paste("r= ", txt, sep = "")
  text(0.5, 0.5, txt)
}

# SPLOM
pairs(
  train[, .(Age, Fare, SibSp, Parch, FMembers, SNCount, nCabins)], 
  col = pal[train$Survived], 
  pch = 20,
  cex = 0.5,
  lower.panel = panel.cor
)
```

For discrete values one should be careful with the interpretation of these 
scatter plots. These few points that can be seen in distinct places are in 
reality many points plotted on top of each other.
The pearson correlation coefficients are still useful though.

For obvious reasons there is a high correlation between *SibSp* and *FMembers*, 
and *Parch* and *FMembers* (created from each other).
There is also a high correlation between *FMembers* and *SNCount*.
The correlation of *SNCount* to either *SibSp* or *Parch* is moderately high.

The remaining feature pairs do not show high correlations.

### Age

Let's see if the age has a substantial effect on survival chances.
Densities are good for such comparisons.
They integrate to 1 which means distributions of
2 features with different total counts can nicely be compared.
Second, they use a kernel for local smoothing.
This way only the local trend is visible.

```{r, warning=FALSE}
ggplot(train, aes(x = Age, fill = Survived)) +
  geom_density(alpha = .5) +
  theme_classic()
```

For both survivors and non-survivors the age distribution looks very similar.
There seems to be just one clear trend change between age 0 and 20.
This is probably because children boarded first.

However, from this plot I can't say whether this separation is really at the 
age of 18.
The proportion difference of survivors and non-survivors in 2 age groups
might be a useful indicator.
I will look at these proportions while adjusting the separation age.

```{r}
dt <- train[!is.na(train$Age), 
            .(isU12 = Age < 12, isU14 = Age < 14,
              isU16 = Age < 16, isU18 = Age < 18,
              isU20 = Age < 20, isU22 = Age < 22,
              Survived)]
dt <- dt[, 
   .(isU12 = sum(isU12) / length(isU12), isU14 = sum(isU14) / length(isU14),
     isU16 = sum(isU16) / length(isU16), isU18 = sum(isU18) / length(isU18),
     isU20 = sum(isU20) / length(isU20), isU22 = sum(isU22) / length(isU22)), 
   by = Survived]
dt <- melt(dt, measure.vars = 2:7, variable.name = "Distinction",
           value.name = "People")
f <- function(x) { x[2] / x[1] }
dt <- dt[, .(Proportion = f(People)), by = Distinction]
ggplot(dt, aes(x = Distinction, y = Proportion, fill = Proportion)) +
  geom_bar(position = "dodge", stat = "identity") + 
  theme_classic()
```

According to this plot, 18 would not be a good age to group children and adults.
Groups should be divided at the age of 14.

```{r}
train$isU14 <- train$Age < 14
mosaicplot(table(train[, .(isU14, Survived)]), shade = TRUE)
```

The mosaic plot visualizes the contingency table of 2 factors as rectangles.
The color here shows the residuals of a model which assumes independence of both
factors.
So, the more colorful the rectangles, the less they can be explained by a 
independence model.
Here, this means feature *isU14* seems to explain the variability in *Survived*
to some degree.

### Families

**SibSp**

To visualize the distribution of discrete features I will use bar plots.

```{r}
ggplot(train, aes(x = SibSp, fill = Survived)) +
  geom_bar(position = "dodge") + 
  theme_classic()
```

Both classes (*survivor*/ *non-survivor*) show similar trends but the 
proportions seem to differ.
This is easier to see in a mosaic plot.

```{r}
mosaicplot(table(train[, .(SibSp, Survived)]), shade = TRUE)
```

It seems that large families in general had lower survival chances.
Passengers with 1 spouse/sibling had the highest survival chances.
But then people with zero spouses/sibling had lower survival chances 
again.

Maybe families in general had higher survival chances because there were 
children involved.
But then for the larger families it was harder to get every one together in the
chaos and that's why they again have lower survival chances.
(my theory for this...)

A linear separator would not be able to distinguish this drop at 0 again.
Therefore, I create a binary feature to only capture the $SibSp = 1$ cases.

```{r}
train$SibSpIs1 <- train$SibSp == 1
mosaicplot(table(train[, .(SibSpIs1, Survived)]), shade = TRUE)
```


**Parch, FMembers, SNCount**

To make it short: There are similar trends in the family related features.
I always extract the cases with highest survival rate as a binary feature.

```{r}
train$ParchIs1 <- train$Parch == 1
train$smallFamily <- train$FMembers > 1 & train$FMembers < 5
train$SNCountIs2 <- train$SNCount == 2
```

### Fare

This feature can be spread out better by a log transformation.
The log10 seems natural for money.

```{r, warning=FALSE}
ggplot(train, aes(x = Fare, fill = Survived)) +
  geom_density(alpha = .5) +
  scale_x_continuous(trans = "log10") +
  theme_classic()
```

It looks like survival chances were higher with a more expensive ticket.
The capitalism is strong with this one.

```{r}
train$lFare <- log10(train$Fare + 1)
```


### Pclass

```{r}
mosaicplot(table(train[, .(Pclass, Survived)]), shade = TRUE)
```

This feature looks useful.
Higher classes had better survial chances.

### Ticket Text

```{r, fig.width=10}
mosaicplot(table(train[, .(TicketText, Survived)]), shade = TRUE)
```

The *PC* label seems to be a useful information.
I will make a feature for this.

```{r}
train$isPC <- train$TicketText == "PC"
```


### Deck

```{r, fig.width=10}
mosaicplot(table(train[, .(Deck, Survived)]), shade = TRUE)
```

Having no deck seems to be an indicator for low survival chances.

```{r}
train$noDeck <- train$Deck == ""
```

### Cabins

I did not see a trend in the number of cabins, only that having no cabin
is a bad omen.

```{r}
mosaicplot(table(train[, .(Survived, hasCabin)]), shade = TRUE)
```

### Embarked

Interestingly, people embarking from Cherbourg had higher survival chances.

```{r}
mosaicplot(table(train[, .(Embarked, Survived)]), shade = TRUE)
train$fromC <- train$Embarked == "C"
```

### Sex

I think this one is obvious.

```{r}
mosaicplot(table(train[, .(Sex, Survived)]), shade = TRUE)
```

### Title

```{r, fig.height=8, fig.width=8}
mosaicplot(table(train[, .(Title, Survived)]), shade = TRUE)
```

Well, needless to say the female titles had higher survival chances.
The interesting thing here is that *Master* and *Rare Title* have different 
survival chances than *Mr*.
Ok *Rare Title* also includes females, but *Master* is a male-only title.

```{r}
mosaicplot(table(train[Title %in% c("Rare Title", "Mr", "Master"), 
                   .(Title, Survived)]), shade = TRUE)
train$isMaster <- train$Title == "Master"
```


### Summary

**Among Features**

+ *FMembers* is highly correlated to *SibSp*, *Parch*, and *SNCount*
+ *SNCount* is moderately correlated to *SibSp* and *Parch*
+ *SibSp* and *Parch* is barely correlated
+ binary *Family* features are correlated among each other
+ *Fare* correlates with *Pclass*
+ *Embarked* is correlated with *TicketText*
+ *isPC* correlates with *Fare*
+ *nCabins* is highly correlated with *Deck*
+ *noDeck* correlates with *Fare*
+ *fromC* is correlated to *hasCabin*
+ *Title* is correlated to *Sex*

**To Target**

+ *Age*: for $Age \geq 14$ similar distributions among classes; for $Age \lt 14$
  more survivors
+ Families as represented by *SibSp*, *Parch*, *FMembers*, *SNCount*: large
  families have lower survival chances
+ *Fare*: higher fares have higher survival chances
+ *Pclass*: higher passenger classes have higher survival chances
+ *isPC*: $TicketText = PC$ means higher survival chances
+ *hasCabin*: having no cabin means lower survival chances
+ *fromC*: people from Cherbourg had higher survival chances
+ *Sex*: females had higher survival chances
+ *isMaster*: higher survival chance with title *Master*






## Missing Values

### Fare

There was 1 $NA$ for *Fare* in the test set.
Since this is just 1 value I think it is reasonable to set it to the median.

```{r, results='hide'}
test[is.na(Fare), Fare := median(c(test$Fare, train$Fare), na.rm = TRUE)]
```

### Embarked

In the train set there were 2 empty strings.
I saw many people label it with *C* because of the high fare (80) of these
passenger tickets.
Below is a violin plot that shows *Fare* densities for the *Embarked* variable.

```{r, warning=FALSE}
ggplot(train[!Embarked == ""], aes(x = Embarked, y = Fare)) +
  geom_violin(fill = "gray") +
  geom_boxplot(alpha = .5, varwidth = TRUE) +
  geom_hline(yintercept = 80, color = "red", linetype = 2) +
  scale_y_continuous(trans = "log10") +
  theme_classic()
```

The y-axis is log transformed to see the spread of *Fare* better.
The red line is drawn for $Fare = 80$.
Clearly, it seems unlikely that the passengers embarked in *Q*.
*C* seems slightly more likely than *S* according to the densities.
However, you can also see the prior probabilities of *Embarked*.
With `varwidth = TRUE` the boxplot widths are proportional to the square roots 
of the groups counts.

```{r}
prop.table(table(as.character(train[!Embarked == "", Embarked])))
```

Taking this into account I would rather set these two values to *S*

```{r, results='hide'}
train[Embarked == "", Embarked := "S"]
```

### Age

There were many $NA$s in *Age* and probably this is an important variable.
At least it is important to know whether or not a passenger was a child.
In other kernels I saw people using the `mice` package.
I didn't know this one before but it sounds quite powerful.

It does multiple imputations using chained equations.
I'm not so familiar with imputation but there are two general ways to do 
imputation.
One is to impute variables one at a time. 
The other (Joint Modeling) is do impute these variables simultaneously.
This package does the first one.
As the name says it does this multiple times (by default 5 times).
This assures that the uncertainty of the data that is imputed is kept.
For the imputation, a set of seamingly useful variables can be defined.

```{r, results='hide', message=FALSE}
full <- rbind(
  train[, .(Age, Pclass, Sex, lFare, Parch, SibSp)],
  test[, .(Age, Pclass, Sex, lFare = log10(Fare + 1), Parch, SibSp)]
)
imp <- mice(full, seed = 42)
```
```{r}
imp
```

The summary tells us that 263 values for *Age* were imputed from *Pclass*, 
*Sex*, *lFare*, *Parch*, and *SibSp*.
The method used was predictive mean matching.
5 datasets have been created.
The plot below shows these datasets.
Blue values in all plots are identical, the red ones were the imputed ones.
The imputed points visibly resemble the distribution of the real points.

```{r}
xyplot(imp, Age ~ lFare | .imp)
```

The idea of the package is to continue fitting the model on all 5 
datasets and then pool the results back together.
Here, I will only use the imputed values of 1 dataset though.

```{r}
impAge <- complete(imp)$Age
```












***










# Modeling

In this section I will create the input matrix ($X$) and fit different models
to it.
To estimate the prediction error of these models cross validation is used.
The best Model will ultimately be chosen for the prediction.



## Create Input Matrix

First, I need to create the input matrix.
I will call it $X$ as in $Y = f(X) + \epsilon$, where $f$ is the model
with input $X$, output $Y$, and error $\epsilon$.
The input matrix has the form $N \times p$ with $N$ rows (the observations)
and $p$ columns (the features or predictor variables).
$Y$ is a class indicator matrix of $N \times 1$ where $0$ indicates 
*non-survivor* and $1$ indicates *survivor*

In the previous section I carelessly created new features.
The assumptions that I made were based on complete data.
Then, I used imputation to replace missing values.
Now, I have to do the feature extraction again in a proper way.
It should include imputed data and it should be aplicable on the test and 
train set.
I will write a function for this.
The function should take the datasets and return a ready-to-use numeric 
input matrix.
For categorical features I have to create dummy variables.

```{r}
MakeX <- function(train, test, 
                  naFare = 80, naEmbarked = "S", naAge = impAge) {
  # combine datasets
  keepCols <- c("PassengerId", "Pclass", "Name", "Sex", "Age", "SibSp", 
                "Parch", "Ticket", "Fare", "Cabin", "Embarked")
  train <- train[, keepCols, with = FALSE]
  test <- test[, keepCols, with = FALSE]
  full <- rbind(train, test)
  
  # impute missing values
  full[is.na(Fare), Fare := naFare]
  full[Embarked == "", Embarked := naEmbarked]
  full[, Age := impAge]
  
  # extract features from Names
  titles <- gsub('(.*, )|(\\..*)', '', full$Name)
  surnames <- vapply(
    full$Name, 
    function(x) strsplit(x, "[,. ]")[[1]][1],
    character(1)
  )
  full$isMaster <- titles == "Master"
  counts <- table(surnames)
  full$SNCount <- vapply(
    surnames, 
    function(sn) counts[names(counts) == sn],
    integer(1)
  )
  
  # extract features from Age
  full$isU14 <- full$Age < 14
  full$isU18 <- full$Age < 18
  
  # dummy for sex
  full$isMale <- full$Sex == "male"
  
  # extract Family features
  full$FMembers <- full$SibSp + full$Parch + 1
  full$SibSpIs1 <- full$SibSp == 1
  full$ParchIs1 <- full$Parch == 1
  full$smallFamily <- full$FMembers > 1 & full$FMembers < 5
  full$SNCountIs2 <- full$SNCount == 2
  
  # transform Fare
  full$lFare <- log10(full$Fare + 1)
  
  # extract Ticket feature
  txt <- toupper(gsub("[0-9\\. /]", "", full$Ticket))
  full$isPC <- txt %in% c("PC", "PPP", "PP")
  
  # extract Cabin features
  full$Deck <- vapply(
    full$Cabin, 
    function(x) strsplit(x, "")[[1]][1],
    character(1)
  )
  full[is.na(Deck), Deck := ""]
  full$noDeck <- full$Deck == ""
  full$nCabins <- vapply(
    full$Cabin, 
    function(x) length(strsplit(x, " ")[[1]]),
    integer(1)
  )
  full$hasCabin <- full$nCabins > 0
  
  # extract Embarked features
  full$fromC <- full$Embarked == "C"
  full$fromQ <- full$Embarked == "Q"
  
  # make X
  keepCols <- c("Pclass", "isMale", "Age", "isU14", "SibSp", "SibSpIs1",
                "Parch", "ParchIs1", "FMembers", "smallFamily", "SNCount",
                "SNCountIs2", "isPC", "Fare", "lFare", "nCabins", "hasCabin",
                "noDeck", "fromC", "fromQ")
  l <- lapply(keepCols, function(colName) { as.integer(full[[colName]]) })
  X <- do.call(cbind, l)
  rownames(X) <- full$PassengerId
  colnames(X) <- keepCols
  
  return(X)
}
```

With this I create X and split it back into test and training again.

```{r}
X <- MakeX(train, test)
Xtrain <- X[1:nrow(train), ]
Xtest <- X[-(1:nrow(train)), ]
Ytrain <- as.integer(as.character(train$Survived))
```

## LDA

A linear discriminant analysis wants to see the data points of each class
as a multivariate Gauss cloud.
It wants to maximize the distances between the clouds' centroids 
with respect to their spreads.
In a regular LDA it is assumed that the covariance matrices of all classes
are the same.
So, for estimating the covariance matrix, all data points are pooled.
With this assumption separating hyperplanes between classes are always linear.

To get an idea what the LDA *sees* we can plot the class distributions along the
1st discriminant coordinate.
For 2 classes this is the orthogonal of the separating hyperplane.
As you can see below, there is a big overlap.
We can exppect a high error rate.

```{r}
fit <- lda(Xtrain, Ytrain)
plot(fit)
```

The warning about colinear variables is what I mentioned during data
preparation.
A variable that is colinear to another variable does not bring a lot of new 
information.
But it brings a lot of variance because its coefficient estimates are unstable.
That's why it makes sense to kick out some variables --
introducing bias in order to reduce variance.

The function below does a forward feature selection.
A model is built stepwise by starting at only an intercept and successivly 
adding a variable.
Variable selection is done by minimizing the error rate, which is in turn
estimated by 10-fold cross validation.
This method is nested and greedy.
I set `improvement = -1` so it will not stop until all variables were included.

```{r, results='hide', message=FALSE, warning=FALSE}
stepsLDA <- stepclass(Xtrain, grouping = Ytrain, 
                   improvement = -1, method = "lda", direction = "forward")
plot(1 - stepsLDA$process$result.pm, type = "b", pch = 20,
     ylab = "error rate", main = "LDA")
```

According to this method there are at least 5 features which I should include 
and at least 5 features which I should not include.
The feature combination with the lowest error rate is...

```{r}
idx <- which.max(stepsLDA$process$result.pm)
as.character(stepsLDA$process$varname[1:idx])
```

Eventually I want to put the model selection process into a cross validation.
Therefore, I write feature selection and the model fitting as a function.
I will do a forward selection again, but this time I set `improvement = 0` to
stop as soon as the error rate goes up again.
I will also reduce set `fold = 5` in `stepclass()` to speed up the whole 
process.

```{r}
predLDA <- function(trainX, trainY, testX, testY) {
  # estimate best feature combination with CV
  steps <- stepclass(
    trainX, grouping = trainY, 
    improvement = 0, method = "lda", direction = "forward",
    fold = 5, output = FALSE
  )
  
  # fit model with selected features
  idx <- which.max(steps$process$result.pm)
  feats <- as.character(steps$process$varname[1:idx])
  trainX <- trainX[, colnames(trainX) %in% feats[-1]]
  fit <- lda(trainX, grouping = trainY)
  
  # make prediction and return accuracy
  testX <- testX[, colnames(testX) %in% feats[-1]]
  pred <- predict(fit, newdata = testX)$class
  acc <- sum(pred == testY) / length(pred)
  return(acc)
}
```

## QDA

The assumption that classes share the same covariance matrix is sometimes
too strict.
If we estimate a covariance matrix for each class, we get a quadratic 
separating hyperplane (therefore QDA).
Now more parameters have to be estimated.

```{r, results='hide', message=FALSE, warning=FALSE}
predQDA <- function(trainX, trainY, testX, testY) {
  # estimate best feature combination with CV
  steps <- stepclass(
    trainX, grouping = trainY, 
    improvement = 0, method = "qda", direction = "forward",
    fold = 5, output = FALSE
  )
  
  # fit model with selected features
  idx <- which.max(steps$process$result.pm)
  feats <- as.character(steps$process$varname[1:idx])
  trainX <- trainX[, colnames(trainX) %in% feats[-1]]
  fit <- qda(trainX, grouping = trainY)
  
  # make prediction and return accuracy
  testX <- testX[, colnames(testX) %in% feats[-1]]
  pred <- predict(fit, newdata = testX)$class
  acc <- sum(pred == testY) / length(pred)
  return(acc)
}
```

## RDA

There is a solution for a continuous shift from QDA to LDA which -- 
a regularized discriminant analysis.
Here you have 2 slide controls.
One parameter adjusts how much of a shared and how much of separate covariance
matrices are used.
Another parameter adjusts to what degree the whole covariance matrix or just
its diagonal is used.
This method is supposed to be more robust to colinearity.

To estimate the best parameter combination RDA also uses cross validation.
This takes quite long. Therefore I will reduce `fold = 3`.

```{r, results='hide', message=FALSE, warning=FALSE}
predRDA <- function(trainX, trainY, testX, testY) {
  # estimate best feature combination with CV
  steps <- stepclass(
    trainX, grouping = trainY, 
    improvement = 0, method = "rda", direction = "forward",
    fold = 3, output = FALSE
  )
  
  # fit model with selected features
  idx <- which.max(steps$process$result.pm)
  feats <- as.character(steps$process$varname[1:idx])
  trainX <- trainX[, colnames(trainX) %in% feats[-1]]
  fit <- rda(trainX, grouping = trainY, fold = 3)
  
  # make prediction and return accuracy
  testX <- testX[, colnames(testX) %in% feats[-1]]
  pred <- predict(fit, newdata = testX)$class
  acc <- sum(pred == testY) / length(pred)
  return(acc)
}
```


## SDA

The last one is a bit more fancy.
It is called shrinkage discriminant analysis.
This is also a regularized linear discriminant analysis but the regularization
method uses several James-Stein type estimators.
This method was actually designed for high dimensionality problems -- 
when you have $p \gg N$.
It also has a feature ranking functionality but I won't use that here.

```{r, results='hide', message=FALSE, warning=FALSE}
predSDA <- function(trainX, trainY, testX, testY) {
  # estimate best feature combination with CV
  steps <- stepclass(
    trainX, grouping = trainY, 
    improvement = 0, method = "sda", direction = "forward",
    fold = 5, output = FALSE, verbose = FALSE
  )
  
  # fit model with selected features
  idx <- which.max(steps$process$result.pm)
  feats <- as.character(steps$process$varname[1:idx])
  trainX <- trainX[, colnames(trainX) %in% feats[-1]]
  fit <- sda(trainX, L = trainY, verbose = FALSE)
  
  # make prediction and return accuracy
  testX <- testX[, colnames(testX) %in% feats[-1]]
  pred <- predict(fit, Xtest = testX, verbose = FALSE)$class
  acc <- sum(pred == testY) / length(pred)
  return(acc)
}
```



## Evaluation

```{r}
CV <- list()
```

Finally, a big cross validation is done to estimate the accuracy of each method
and its standard error.
I will use the `crossval` function for this.
It takes care of preserving class proportions when splitting data.

By default this repeats a 10-fold cross validation 20 times.
To reduce time, I set it to 3-fold and 5 times.
The following functions can take some minutes.
(Be warned, I can't make `sda` shut up!)

```{r, warning=FALSE, results='hide', message=FALSE}
# LDA
CV$LDA <- crossval(predLDA, Xtrain, Ytrain, 
                   verbose = FALSE, K = 3, B = 5)

# QDA
CV$QDA <- crossval(predQDA, Xtrain, Ytrain, 
                   verbose = FALSE, K = 3, B = 5)

# RDA
#CV$RDA <- crossval(predRDA, Xtrain, Ytrain, 
#                   verbose = FALSE, K = 3, B = 5)

# QDA
CV$SDA <- crossval(predSDA, Xtrain, Ytrain, 
                   verbose = FALSE, K = 3, B = 5)
```

I commented *RDA* out because it takes to long to run on Kaggle.
(Didn't know this before)
It was not the method with the best accuracy so no problem.

The plot below summarizes the results.
*QDA* wins with more than 81% predicted accuracy.
This method will be chosen for the final prediction.

```{r, fig.width=8}
l <- lapply(CV, function(i) { data.frame(Acc = i$stat, se = i$stat.se) })
res <- do.call(rbind, l)
res$Model <- rownames(res)
ggplot(res, aes(x = Model, y = Acc)) +
  geom_pointrange(aes(ymax = Acc + se, ymin = Acc - se), color = "blue") +
  coord_flip() +
  ggtitle("Predicted Accuracy with Standard Error") +
  theme_classic()
```

  
  
  
  


  
***











# Prediction

## Final Model

We estimated that the *QDA* procedure with forward feature selection gives 
the best prediction accuracy.
Now this is applied on the whole training set.

```{r}
# estimate best feature combination with CV
steps <- stepclass(
  Xtrain, grouping = Ytrain, 
  improvement = 0, method = "qda", direction = "forward",
  fold = 5, output = FALSE
)

# fit model with selected features
idx <- which.max(steps$process$result.pm)
feats <- as.character(steps$process$varname[1:idx])
redXtrain <- Xtrain[, colnames(Xtrain) %in% feats[-1]]
fit <- qda(redXtrain, grouping = Ytrain)
```

We can look at the final model.
There is a prior of roughly 60% non-survivors to 40% survivors.
From the group means table you can see which features are used.

```{r}
fit
```

## Predict Test Set

Now the model is finally used on the test dataset.

```{r}
redXtest <- Xtest[, colnames(Xtest) %in% feats[-1]]
pred <- predict(fit, newdata = redXtest)
```

The posterior probability distribution is shown below.
For each passenger the class with the highest probability is chosen.
As you can see the model is quite sure about its decision.
There are no cases near 0.5.

```{r}
df <- data.frame(
  Prob = c(pred$posterior[, 1], pred$posterior[, 2]),
  Class = rep(c("Non-Survivor", "Survivor"), each = nrow(pred$posterior))
)
ggplot(df, aes(x = Prob)) +
  geom_histogram(bins = 20) +
  facet_grid(Class ~ .) +
  theme_bw()
```

These are the predictions...

```{r}
df <- data.frame(
  PassengerId = rownames(pred$posterior),
  Survived = pred$class
)
write.csv(df, file = "QDA_submission.csv", row.names = FALSE, quote = FALSE)
table(df$Survived)
```

That's it!
Thanks for reading through this whole thing.
If you have any suggestions, comments are welcome.

***


