---
title: "Housing Cri.. Prices"
subtitle: "Model MARS"
author: "Marc Schwering"
output:
  html_document:
    number_sections: true
    toc: true
    toc_depth: 2
    fig_width: 10
    fig_height: 7
    theme: cosmo
---









# Pre-Processing

The following steps prepare the dataset for the modeling part.
The actual feature analysis was done in another kernel.

**Loading Data**

```{r, message=FALSE, warning=FALSE, results='hide'}
library(data.table) # database-like tables
library(Matrix) # sparse matrices
library(ggplot2) # general plots
library(mice) # imputation
library(xgboost) # xtreme gradient boosting
library(crossval) # doing CV
test <- data.table(read.csv("../input/test.csv"))
train <- data.table(read.csv("../input/train.csv"))
test$SalePrice <- NaN
full <- rbind(train, test)
full$label <- rep(c("train", "test"), c(nrow(train), nrow(test)))
```

**Feature Attributes**

```{r, results='hide'}
# types
nomVars <- c("MSZoning", "Street", "Alley", "LandContour", "LotConfig",
             "Neighborhood", "Condition1", "Condition2", "HouseStyle",
             "RoofStyle", "RoofMatl", "Exterior1st", "Exterior2nd",
             "MasVnrType", "Foundation", "Heating", "CentralAir", "Electrical",
             "GarageType", "PavedDrive", "MiscFeature", "SaleType",
             "SaleCondition")
ordVars <- c("MSSubClass", "LotShape", "Utilities", "LandSlope", "BldgType",
             "OverallQual", "OverallCond", "ExterQual", "ExterCond", "BsmtQual",
             "BsmtCond", "BsmtExposure", "BsmtFinType1", "BsmtFinType2",
             "HeatingQC", "KitchenQual", "Functional", "FireplaceQu",
             "GarageFinish", "GarageQual", "GarageCond", "PoolQC", "Fence",
             "MoSold")
discVars <- c("YearBuilt", "YearRemodAdd", "BsmtFullBath", "BsmtHalfBath",
              "FullBath", "HalfBath", "BedroomAbvGr", "KitchenAbvGr",
              "TotRmsAbvGrd", "Fireplaces", "GarageYrBlt", "GarageCars",
              "YrSold")
contVars <- c("LotArea", "MasVnrArea", "BsmtFinSF1", "BsmtFinSF2", "BsmtUnfSF",
              "TotalBsmtSF", "X1stFlrSF", "X2ndFlrSF", "LowQualFinSF",
              "GrLivArea", "GarageArea", "WoodDeckSF", "OpenPorchSF",
              "EnclosedPorch", "X3SsnPorch", "ScreenPorch", "PoolArea",
              "MiscVal", "LotFrontage")
featAttr <- data.table(
  name = colnames(test)[-1],
  type = "nominal"
)
featAttr[name %in% ordVars, type := "ordinal"]
featAttr[name %in% discVars, type := "discrete"]
featAttr[name %in% contVars, type := "continuous"]

# special values
full[is.na(Alley), Alley := "noAccess"]
full[is.na(BsmtQual), BsmtQual := "noBasement"]
full[is.na(BsmtCond), BsmtCond := "noBasement"]
full[is.na(BsmtExposure), BsmtExposure := "noBasement"]
full[is.na(BsmtFinType1), BsmtFinType1 := "noBasement"]
full[is.na(BsmtFinType2), BsmtFinType2 := "noBasement"]
full[is.na(FireplaceQu), FireplaceQu := "noFireplace"]
full[is.na(GarageType), GarageType := "noGarage"]
full[is.na(GarageFinish), GarageFinish := "noGarage"]
full[is.na(GarageQual), GarageQual := "noGarage"]
full[is.na(GarageCond), GarageCond := "noGarage"]
full[is.na(PoolQC) , PoolQC := "noPool"]
full[is.na(Fence), Fence := "noFence"]
full[is.na(MiscFeature), MiscFeature := "none"]
full[GarageYrBlt == 2207, GarageYrBlt := 2007]

# variable types
for (j in c(nomVars, ordVars)) full[[j]] <- as.factor(full[[j]])
for (j in contVars) full[[j]] <- as.numeric(full[[j]])
for (j in discVars) full[[j]] <- as.integer(full[[j]])

# order levels
lvlOrd <- list(
  LotShape = c("Reg", "IR1", "IR2", "IR3"),
  Utilities = c("AllPub", "NoSewr", "NoSeWa", "ELO"),
  LandSlope = c("Gtl", "Mod", "Sev"),
  BldgType = c("1Fam", "2FmCon", "Duplx", "TwnhsE", "TwnhsI"),
  OverallQual = 10:1,
  OverallCond = 10:1,
  ExterQual = c("Ex", "Gd", "TA", "Fa", "Po"),
  ExterCond = c("Ex", "Gd", "TA", "Fa", "Po"),
  BsmtQual = c("Ex", "Gd", "TA", "Fa", "Po", "noBasement"),
  BsmtCond = c("Ex", "Gd", "TA", "Fa", "Po", "noBasement"),
  BsmtExposure = c("Gd", "Av", "Mn", "No", "noBasement"),
  BsmtFinType1 = c("GLQ", "ALQ", "BLQ", "Rec", "LwQ", "Unf", "noBasement"),
  BsmtFinType2 = c("GLQ", "ALQ", "BLQ", "Rec", "LwQ", "Unf", "noBasement"),
  HeatingQC = c("Ex", "Gd", "TA", "Fa", "Po"),
  KitchenQual = c("Ex", "Gd", "TA", "Fa", "Po"),
  Functional = c("Typ", "Min1", "Min2", "Mod", "Maj1", "Maj2", "Sev", "Sal"),
  FireplaceQu = c("Ex", "Gd", "TA", "Fa", "Po", "noFireplace"),
  GarageFinish = c("Fin", "RFn", "Unf", "noGarage"),
  GarageQual = c("Ex", "Gd", "TA", "Fa", "Po", "noGarage"),
  GarageCond = c("Ex", "Gd", "TA", "Fa", "Po", "noGarage"),
  PoolQC = c("Ex", "Gd", "TA", "Fa", "noPool"),
  Fence = c("GdPrv", "MnPrv", "GdWo", "MnWw", "noFence"),
  MoSold = 1:12
)
for (j in names(lvlOrd)) {
  full[[j]] <- factor(full[[j]], levels = lvlOrd[[j]])
}

# undefined values
full[BsmtCond == "noBasement" & is.na(BsmtUnfSF), BsmtUnfSF := 0]
full[BsmtCond == "noBasement" & is.na(TotalBsmtSF), TotalBsmtSF := 0]
full[BsmtFinType1 == "noBasement" & is.na(BsmtFinSF1), BsmtFinSF1 := 0]
full[BsmtFinType2 == "noBasement" & is.na(BsmtFinSF2), BsmtFinSF2 := 0]
full[GarageType == "noGarage" & is.na(GarageYrBlt), GarageYrBlt := 0]
full[GarageType == "noGarage" & is.na(GarageCars), GarageCars := 0]
full[GarageType == "noGarage" & is.na(GarageArea), GarageArea := 0]
full[PoolQC == "noPool" & is.na(PoolArea), PoolArea := 0]
full[MiscFeature == "noFeature" & is.na(MiscVal), MiscVal := 0]
```

**Missing Values**

```{r, results='hide'}
# single imputation
full[c(949, 1488, 2349), BsmtExposure := "No"]
full[c(2041, 2186, 2525), BsmtCond := "TA"]
full[c(2218, 2219), BsmtQual := "TA"]
full[c(2127, 2577), GarageYrBlt := 0]
full[c(2127, 2577), GarageCars := 0]
full[c(2127, 2577), GarageArea := 0]
full[c(2127, 2577), GarageType := "noGarage"]
full[is.na(Exterior1st), Exterior1st := "VinylSd"]
full[is.na(Exterior2nd), Exterior2nd := "VinylSd"]
full[is.na(Electrical), Electrical := "SBrkr"]
full[is.na(KitchenQual), KitchenQual := "TA"]
full[is.na(SaleType), SaleType := "WD"]
full[is.na(Utilities), Utilities := "AllPub"]
full[is.na(Functional), Functional := "Typ"]
full[is.na(MSZoning), MSZoning := "RL"]
full[is.na(MasVnrArea), MasVnrArea := 202]
full[is.na(MasVnrType), MasVnrType := "none"]
full[is.na(BsmtFullBath), BsmtFullBath := 0]
full[is.na(BsmtHalfBath), BsmtHalfBath := 0]

# transform
full$SalePrice <- log(full$SalePrice)
for (j in contVars) {
  full[[j]] <- log10(full[[j]] + 1)
}

# multiple imputation
vars <- c(contVars, discVars, "BldgType", "MSSubClass", "HouseStyle")
imp <- mice(full[, vars, with = FALSE], seed = 42)
full$BldgType <- complete(imp)$BldgType
full$LotFrontage <- complete(imp)$LotFrontage
```

**Important Features**

```{r}
impVars <- c("MSSubClass", "MSZoning", "LotFrontage", "LotArea", "LandContour", 
              "Neighborhood", "BldgType", "HouseStyle", "OverallQual", 
              "OverallCond", "YearBuilt", "YearRemodAdd", "Exterior1st", 
              "Exterior2nd", "MasVnrArea", "ExterQual", "Foundation", 
              "BsmtQual", "BsmtCond", "BsmtExposure", "BsmtFinType1", 
              "BsmtFinSF1", "BsmtUnfSF", "TotalBsmtSF", "HeatingQC", 
              "CentralAir", "X1stFlrSF", "X2ndFlrSF", "GrLivArea", 
              "BsmtFullBath", "FullBath", "HalfBath", "BedroomAbvGr", 
              "KitchenAbvGr", "KitchenQual", "TotRmsAbvGrd", "Fireplaces", 
              "FireplaceQu", "GarageType", "GarageYrBlt", "GarageFinish", 
              "GarageCars", "GarageArea", "GarageQual", "GarageCond", 
              "PavedDrive", "WoodDeckSF", "OpenPorchSF")
```

**Adjust Features**

```{r, results='hide'}
# binary features
full$noAccess <- full$Alley == "noAccess"
full$noBasement <- full$BsmtQual == "noBasement" | full$BsmtCond == "noBasement"
full$noFireplace <- full$FireplaceQu == "noFireplace"
full$noGarage <- full$GarageType == "noGarage" | full$GarageCond == "noGarage"
full$noPool <- full$PoolQC == "noPool"
full$noFence <- full$Fence == "noFence"
full$noMisc <- full$MiscFeature == "none"
binVars <- c("noAccess", "noBasement", "noFireplace", "noGarage", 
              "noPool", "noFence", "noMisc")

# ordinal to integer
for (j in ordVars) {
  full[[j]] <- as.integer(full[[j]])
}
```













***




















# Modeling



## Preparation

The dataset is split back into training and test.

```{r}
train <- full[label == "train", !c("Id", "label")]
rownames(train) <- full[label == "train", Id]
test <- full[label == "test", !c("Id", "label", "SalePrice")]
rownames(test) <- full[label == "test", Id]
```

Categorical variables are still coded as factors.
Here, they are split up into binary dummy variables.

```{r}
X <- sparse.model.matrix(~ -1 + ., data = train[, !"SalePrice"])
Y <- train$SalePrice
```


## Model Selection

To see overfitting in the training, the training set must be split up again
into a validation and a training set.
The model will be repeatedly fitted on the training set, the progress is
observed on the performance of the fitted model on the validation set.
Here, a random 25% validation 75% training set split is chosen.
The training set should be big to enable a good training,
but the validation set must still be big enough 
to capture structures within the data and correctly estimate the 
performance of the fitted model.
> I hope 25% is still big enough on 1460 data points

```{r}
set.seed(42)
split <- sample(1:nrow(X), round(nrow(X) / 4))
dtrain <- xgb.DMatrix(data = X[-split, ], label = Y[-split])
dval <- xgb.DMatrix(data = X[split, ], label = Y[split])
```

### Quadratic Loss

Regression trees will be the base learners of this boosting model.
The function to optimize is the *objective* which has a usual loss function
and a function to penalize complexity.
In the linear regression setting the loss function is a quadratic loss.

> Doc says 'linear regression' so I assume it's a quadratic loss.

In a MART fashion the algorithm will add one tree after another, fitting
the new trew to the *pseudo-residuals* of the previous one.
Regularization for this process comes in many forms.

Parameter $\eta$ (the stepsize) reduces the influence of each newly learned 
tree. 
It is closely related to $L_1$ regularization in linear regression.
$\eta$ is usually set very small $<0.1$.
The smaller $\eta$ is, the slower the model learns.
So, more iterations are needed as well.
The strategy here is to compute enough iterations for a given $\eta$,
watch the validation error, and then chose the number of iterations
with the minimum validation error.

> I start with 10000 iterations and eta = 0.01 

The maximum allowed tree depth per model is another parameter
which can limit the complexity of the model.
High maximum tree depth (> 6) would represent interactions of many 
variables in the data.
A MARS model that I fitted on the data earlier suggested no interactions.
(Additive models did perform better than models with interaction terms)
Therefore, small trees or stumps seem to be more approprate here.

> I will look around a maxdepth of 2 or 4

Some additional parameter which reduces overfitting is the subsampling rate.
This is the proportion of data points which get sampled 
(without replacement) before fitting each tree.
A usual value here is 50%.
This reduces the learning process as well, but it also speeds up computation.
Here is one initial run:

```{r, results='hide'}
fit <- xgb.train(booster = "gbtree", data = dtrain,  eta = .01, max.depth = 4, 
                 subsample = .5, nround = 10000, objective = "reg:linear",
                 watchlist = list(train = dtrain, test = dval))
```

By default `xgb.train` already computes RMSE on the training and validation set
in the lienar regression setting.
Below is a plot of the learning process.
10000 iterations seem sufficient to reach minimum test error 
when $\eta = 0.01$ and 50% subsampling is used.

```{r, warning=FALSE}
dt <- melt(fit$evaluation_log, "iter", c("train_rmse", "test_rmse"))
ggplot(dt) + 
  geom_path(aes(x = iter, y = value, color = variable)) + 
  scale_y_continuous(limits = c(0, .2)) +
  theme_bw()
```

There are some tools to inspect the model.
The plot below shows the number of leafs and weighted number of observations 
(cover) which end up at certain tree depth.
With this settings trees are mostly built to full (maximum) length.


One can prevent the formation of leafs by setting a minimum child weight.
A split would not be performed if both partitions would not get a minimum
number of observations.

> As far as I know, in the regression setting the number of observations ending
up in a single leaf can be directly translated to child weight.

Appropriate numbers for minimum child weight have to be found.
From the plot below, it seems that for a maximum tree depth of 4 
it would not make sense to look for minimum child weights above around 150.

```{r}
xgb.plot.deepness(model = fit)
```

There is another parameter $\gamma$ which prevents trees from growing.
It only allows to further grow a tree if a minimum increase in RMSE (here)
is achieved by the partition.
Looking at the RMSE differences of the run above, a very low $\gamma$ seems
appropriate.

Then, there are some parameters where I have no idea where to start.
One is $\alpha$, which is a $L_1$ regularization for weights.
The other is $colsample_bytree$.
This is another sampling method which samples a proportion of 
variables (columns in $X$) before building each tree.

> I will just try out some vlaues here...

This is the final parameter grid:

```{r}
pars.grid <- expand.grid(
  eta = c(0.01, 0.005),
  gamma = c(0, 0.01),
  max_depth = c(2, 4),
  min_child_weight = c(1, 50, 100),
  alpha = c(0, 0.5),
  colsample_bytree = c(1, 0.8)
)
```

Each parameter combination will be trained and the lowest RMSE and its 
iteration number for each parameter combination will be returned.
An early stopping condition is added, so that the process stops if
after 1000 iterations there has not been any decrease in RMSE.

```{r}
train.grid <- function(train, val, pars, n = 10000, subs = .5,
                       bstr = "gbtree", objFun = NULL) {
  N <- nrow(pars)
  out <- data.frame(iter = integer(N), rmse = numeric(N))
  for (i in seq_len(N)) {
    fit <- xgb.train(booster = bstr, data = train, eta = pars$eta[i], 
                     gamma = pars$gamma[i], max.depth = pars$max_depth[i], 
                     min_child_weight = pars$min_child_weight[i], 
                     subsample = subs, alpha = pars$alpha[i], 
                     colsample_bytree = pars$colsample_bytree[i], 
                     nround = n, watchlist = list(train = dtrain, test = dval), 
                     obj = objFun, early_stopping_rounds = 1000)  
    dt <- fit$evaluation_log
    idx <- which.min(dt$test_rmse)
    out$iter[i] <- idx
    out$rmse[i] <- dt$test_rmse[idx]  
  }
  return(out)
}
```

The training is done below.
For the Kaggle kernel this is commented. (It will run a few minutes)
The code below summarizes the results of all parameter combinations as
a plot.

```{r, results='hide'}
res <- train.grid(dtrain, dval, pars.grid)
```

```{r, results='hide', echo=FALSE}
saveRDS(res, "res1.rds")
```

```{r}
df <- cbind(res, pars.grid)
df[which.min(df$rmse), ]
ggplot(df, aes(x = min_child_weight, y = rmse)) +
 geom_point(aes(color = as.factor(eta), shape = as.factor(alpha))) +
 facet_grid(max_depth ~ gamma + colsample_bytree) +
 theme_bw()
```

On the plot above the effects of different parameters are nicely visible.
Tree depth 2 is overall better than 4, 
increasing minimum child weight increased RMSE,
$\eta$ of 0.01 is better than 0.005 (maybe that would need more iterations),
$L_1$ regularization increased error overall,
a $\gamma$ of 0.01 did slightly better than 0,
in some cases the 80% column sampling by tree decreased RMSE slightly.
The differences in RMSE for minimum child weight seem most drastic with
$\Delta$RMSE of 0.01, followed by $\eta$ with $\Delta$RMSE of around 0.005.
The best parameter combination is:

```{r}
pars1 <- list(
  iter = 7366,
  eta = 0.01,
  gamma = 0.01,
  max_depth = 2,
  min_child_weight = 1,
  alpha = 0,
  colsample_bytree = 0.8
)
```

### Huber Loss

Quadratic loss can be sensitive to outliers.
Other loss functions, such as the Huber loss, 
become linear with increasing residual vlaues.
`xgb.train` allows to provide custom loss functions.

The function below should describe a Huber loss.
It takes predictions and the training object
and returns first and second order gradients.

```{r}
Huber <- function(preds, dtrain) {
  labels <- getinfo(dtrain, "label")
  grad <- ifelse(abs(preds - labels < 1.5),
                 preds - labels,
                 1.5 * sign(preds - labels))
  hess <- ifelse(abs(preds - labels < 1.5), 1, 0)
  return(list(grad = grad, hess = hess))
}
```

The same parameter grid as above is trained.
Again, it is commented for the Kaggle kernel.

```{r, results='hide'}
res <- train.grid(dtrain, dval, pars.grid, objFun = Huber)
```

```{r, results='hide', echo=FALSE}
saveRDS(res, "res2.rds")
```

```{r}
df <- cbind(res, pars.grid)
df[which.min(df$rmse), ]
ggplot(df, aes(x = min_child_weight, y = rmse)) +
  geom_point(aes(color = as.factor(eta), shape = as.factor(alpha))) +
  facet_grid(max_depth ~ gamma + colsample_bytree) +
  theme_bw()
```

The results are the same.
For small residuals the Huber and Quadratic loss are identical.
It seems that the Huber loss was not really necessary here.

```{r}
pars2 <- list(
  iter = 7366,
  eta = 0.01,
  gamma = 0.01,
  max_depth = 2,
  min_child_weight = 1,
  alpha = 0,
  colsample_bytree = 0.8
)
```


### RMSE Estimation

To correctly estimate the RMSE of this method, the whole procedure
(including grid search and parameter selection)
would need to be plugged into a repeated cross-validation.
Again, this is not feasible in the Kaggle kernel.

Here, only the two models quadratic loss with `pars1` 
and Huber loss with `pars2` are compared in a cross validation.
Since these were derived from 75% of the whole training set
the RMSE predictions will be biased downward (better RMSE).
Nevertheless, the results can be used to chose one of the two methods.
For that the full training set is restored and parameters are 
combined in a list.

```{r}
dtrain <- xgb.DMatrix(data = X, label = Y)
pars0 <- list(
  subsample = .5,
  booster = "gbtree"
)
pars <- c(pars0, pars1)
```

Inside the cross validation, the model is fit with the specified parameters
for the specified number of rounds.

```{r}
RMSE <- function(Y, Yh) sqrt(sum((Y - Yh)^2) / length(Y))
predXGB <- function(trainX, trainY, testX, testY, ...) {
  dtrain <- xgb.DMatrix(data = trainX, label = trainY)
  fit <- xgb.train(params = pars, nrounds = pars$iter, data = dtrain, ...) 
  pred <- predict(fit, testX)
  return(RMSE(testY, pred))
}
```

The 5-fold cross validation is repeated 3 times.
It returns mean RMSE and its standard error.

```{r, results='hide'}
CV <- list()
CV$Quad <- crossval(predXGB, X, Y, K = 5, B = 3)
```

The same is repeated for the Huber loss model.

```{r, results='hide'}
pars <- c(pars0, pars2)
CV$Huber <- crossval(predXGB, X, Y, K = 5, B = 3, obj = Huber)
```

Mean RMSEs and their standard error are shown below.

```{r}
l <- lapply(CV, function(i) data.frame(RMSE = i$stat, se = i$stat.se))
res <- do.call(rbind, l)
res$Model <- rownames(res)
ggplot(res, aes(x = Model, y = RMSE)) +
  geom_pointrange(aes(ymax = RMSE + se, ymin = RMSE - se), color = "blue") +
  coord_flip() +
  ggtitle("CV-Estimated RMSE with Standard Error") +
  theme_classic()
```

















***
