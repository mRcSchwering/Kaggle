---
title: "Housing Cri.. Prices"
subtitle: "Model MARS"
author: "Marc Schwering"
output:
  html_document:
    number_sections: true
    toc: true
    toc_depth: 2
    fig_width: 10
    fig_height: 7
    theme: cosmo
---









# Pre-Processing

The following steps prepare the dataset for the modeling part.
The actual feature analysis was done in another kernel.

**Loading Data**

```{r, message=FALSE, warning=FALSE, results='hide'}
library(data.table) # database-like tables
library(ggplot2) # general plots
library(vcd) # stats for categories
library(mice) # imputation
library(earth) # MARS
library(crossval) # doing CV
test <- data.table(read.csv("../input/test.csv"))
train <- data.table(read.csv("../input/train.csv"))
test$SalePrice <- NaN
full <- rbind(train, test)
full$label <- rep(c("train", "test"), c(nrow(train), nrow(test)))
```

**Feature Attributes**

```{r, results='hide'}
# types
nomVars <- c("MSZoning", "Street", "Alley", "LandContour", "LotConfig",
             "Neighborhood", "Condition1", "Condition2", "HouseStyle",
             "RoofStyle", "RoofMatl", "Exterior1st", "Exterior2nd",
             "MasVnrType", "Foundation", "Heating", "CentralAir", "Electrical",
             "GarageType", "PavedDrive", "MiscFeature", "SaleType",
             "SaleCondition")
ordVars <- c("MSSubClass", "LotShape", "Utilities", "LandSlope", "BldgType",
             "OverallQual", "OverallCond", "ExterQual", "ExterCond", "BsmtQual",
             "BsmtCond", "BsmtExposure", "BsmtFinType1", "BsmtFinType2",
             "HeatingQC", "KitchenQual", "Functional", "FireplaceQu",
             "GarageFinish", "GarageQual", "GarageCond", "PoolQC", "Fence",
             "MoSold")
discVars <- c("YearBuilt", "YearRemodAdd", "BsmtFullBath", "BsmtHalfBath",
              "FullBath", "HalfBath", "BedroomAbvGr", "KitchenAbvGr",
              "TotRmsAbvGrd", "Fireplaces", "GarageYrBlt", "GarageCars",
              "YrSold")
contVars <- c("LotArea", "MasVnrArea", "BsmtFinSF1", "BsmtFinSF2", "BsmtUnfSF",
              "TotalBsmtSF", "X1stFlrSF", "X2ndFlrSF", "LowQualFinSF",
              "GrLivArea", "GarageArea", "WoodDeckSF", "OpenPorchSF",
              "EnclosedPorch", "X3SsnPorch", "ScreenPorch", "PoolArea",
              "MiscVal", "LotFrontage")
featAttr <- data.table(
  name = colnames(test)[-1],
  type = "nominal"
)
featAttr[name %in% ordVars, type := "ordinal"]
featAttr[name %in% discVars, type := "discrete"]
featAttr[name %in% contVars, type := "continuous"]

# special values
full[is.na(Alley), Alley := "noAccess"]
full[is.na(BsmtQual), BsmtQual := "noBasement"]
full[is.na(BsmtCond), BsmtCond := "noBasement"]
full[is.na(BsmtExposure), BsmtExposure := "noBasement"]
full[is.na(BsmtFinType1), BsmtFinType1 := "noBasement"]
full[is.na(BsmtFinType2), BsmtFinType2 := "noBasement"]
full[is.na(FireplaceQu), FireplaceQu := "noFireplace"]
full[is.na(GarageType), GarageType := "noGarage"]
full[is.na(GarageFinish), GarageFinish := "noGarage"]
full[is.na(GarageQual), GarageQual := "noGarage"]
full[is.na(GarageCond), GarageCond := "noGarage"]
full[is.na(PoolQC) , PoolQC := "noPool"]
full[is.na(Fence), Fence := "noFence"]
full[is.na(MiscFeature), MiscFeature := "none"]
for (j in c(nomVars, ordVars, discVars)) full[[j]] <- as.factor(full[[j]])
for (j in contVars) full[[j]] <- as.numeric(full[[j]])

# order levels
lvlOrd <- list(
  LotShape = c("Reg", "IR1", "IR2", "IR3"),
  Utilities = c("AllPub", "NoSewr", "NoSeWa", "ELO"),
  LandSlope = c("Gtl", "Mod", "Sev"),
  BldgType = c("1Fam", "2FmCon", "Duplx", "TwnhsE", "TwnhsI"),
  OverallQual = 10:1,
  OverallCond = 10:1,
  ExterQual = c("Ex", "Gd", "TA", "Fa", "Po"),
  ExterCond = c("Ex", "Gd", "TA", "Fa", "Po"),
  BsmtQual = c("Ex", "Gd", "TA", "Fa", "Po", "noBasement"),
  BsmtCond = c("Ex", "Gd", "TA", "Fa", "Po", "noBasement"),
  BsmtExposure = c("Gd", "Av", "Mn", "No", "noBasement"),
  BsmtFinType1 = c("GLQ", "ALQ", "BLQ", "Rec", "LwQ", "Unf", "noBasement"),
  BsmtFinType2 = c("GLQ", "ALQ", "BLQ", "Rec", "LwQ", "Unf", "noBasement"),
  HeatingQC = c("Ex", "Gd", "TA", "Fa", "Po"),
  KitchenQual = c("Ex", "Gd", "TA", "Fa", "Po"),
  Functional = c("Typ", "Min1", "Min2", "Mod", "Maj1", "Maj2", "Sev", "Sal"),
  FireplaceQu = c("Ex", "Gd", "TA", "Fa", "Po", "noFireplace"),
  GarageFinish = c("Fin", "RFn", "Unf", "noGarage"),
  GarageQual = c("Ex", "Gd", "TA", "Fa", "Po", "noGarage"),
  GarageCond = c("Ex", "Gd", "TA", "Fa", "Po", "noGarage"),
  PoolQC = c("Ex", "Gd", "TA", "Fa", "noPool"),
  Fence = c("GdPrv", "MnPrv", "GdWo", "MnWw", "noFence"),
  MoSold = 1:12
)
for (j in names(lvlOrd)) {
  full[[j]] <- factor(full[[j]], levels = lvlOrd[[j]])
}

# undefined values
full[BsmtCond == "noBasement" & is.na(BsmtUnfSF), BsmtUnfSF := 0]
full[BsmtCond == "noBasement" & is.na(TotalBsmtSF), TotalBsmtSF := 0]
full[BsmtFinType1 == "noBasement" & is.na(BsmtFinSF1), BsmtFinSF1 := 0]
full[BsmtFinType2 == "noBasement" & is.na(BsmtFinSF2), BsmtFinSF2 := 0]
full[GarageType == "noGarage" & is.na(GarageYrBlt), GarageYrBlt := "0"]
full[GarageType == "noGarage" & is.na(GarageCars), GarageCars := "0"]
full[GarageType == "noGarage" & is.na(GarageArea), GarageArea := 0]
full[PoolQC == "noPool" & is.na(PoolArea), PoolArea := 0]
full[MiscFeature == "noFeature" & is.na(MiscVal), MiscVal := 0]
```

**Missing Values**

```{r, results='hide'}
# single imputation
full[c(949, 1488, 2349), BsmtExposure := "No"]
full[c(2041, 2186, 2525), BsmtCond := "TA"]
full[c(2218, 2219), BsmtQual := "TA"]
full[c(2127, 2577), GarageYrBlt := "0"]
full[c(2127, 2577), GarageCars := "0"]
full[c(2127, 2577), GarageArea := 0]
full[c(2127, 2577), GarageType := "noGarage"]
full[is.na(Exterior1st), Exterior1st := "VinylSd"]
full[is.na(Exterior2nd), Exterior2nd := "VinylSd"]
full[is.na(Electrical), Electrical := "SBrkr"]
full[is.na(KitchenQual), KitchenQual := "TA"]
full[is.na(SaleType), SaleType := "WD"]
full[is.na(Utilities), Utilities := "AllPub"]
full[is.na(Functional), Functional := "Typ"]
full[is.na(MSZoning), MSZoning := "RL"]
full[is.na(MasVnrArea), MasVnrArea := 202]
full[is.na(MasVnrType), MasVnrType := "none"]
full[is.na(BsmtFullBath), BsmtFullBath := "0"]
full[is.na(BsmtHalfBath), BsmtHalfBath := "0"]

# multiple imputation
vars <- c(contVars, "BldgType", "MSSubClass", "BedroomAbvGr", "TotRmsAbvGrd")
imp <- mice(full[, vars, with = FALSE], seed = 42)
full$BldgType <- complete(imp)$BldgType
full$LotFrontage <- complete(imp)$LotFrontage
```

















***









# Modeling



## Preparation

The dataset is split back into training and test.

```{r}
train <- full[label == "train", !"label"]
test <- full[label == "test", !c("label", "SalePrice")]
X <- train[, !"SalePrice"]
Y <- train$SalePrice
```

The target variable is log-transformed.
During *Feature Exploration* continuous variables showed a broader spread
when log10 transformed.
Therefore a second input matrix with log10 transformed continuous variables
is created.

```{r}
lY <- log(Y)
lX <- X
for (j in contVars) {
  lX[[j]] <- log10(lX[[j]] + 1)
}
```



## Training

In general the forward pass should create a model of sufficiently high
complexity.
The pruning step should then simplify the model again to reduce overfitting.
During the forward pass there are a few conditions which define when the
model has reached a high enough complexity.
Two of them are a maximum number of terms and a threshold in residual sum of
squares gain (*RSS*).
Here, the maximum number of terms is always set very high (200).
Thereby, the forward pass only stops when no increase in *RSS* by more than
0.001 is gained.

The models performance on the training dataset can be read mainly from
the $R^2$ (*RSq*) and generalized $R^2$ (*GRSq*).
In *GRSq* there is a penalty for model complexity which is estimated by
the number of terms, knots and interactions in the model.
Furthermore residual and quantile-quantile plots, 
and cummulative distributions of residuals
can be used to see the model's performance.

The maximum *GRSq* value is also used for finding an appropriate model size
during the pruning step.
Instead of using the *GRSq*, a cross-validation estimated $R^2$ can be used
as well.
However, this is computationally too expensive for this kernel.
*GRSq* usually finds roughly the same model size anyway.

**Additive**

First, an additive model of hinge functions is created by not allowing
interaction terms.

```{r}
fit.Add <- earth(X, lY, nk = 200)
fit.Add
plot(fit.Add)
```

**Additive + log X**

First, an additive model of hinge functions is created by not allowing
interaction terms.

```{r}
fit.lAdd <- earth(lX, lY, nk = 200)
fit.lAdd
plot(fit.lAdd)
```

**Interaction**

Next, 2nd degree interaction terms are allowed.

```{r}
fit.lInt <- earth(X, lY, nk = 200, degree = 2)
fit.lInt
plot(fit.lInt)
```

**Interaction + log X**

Feature exploration suggested log10 transformed continuous variables
which are used here.

```{r}
fit.lInt <- earth(lX, lY, nk = 200, degree = 2)
fit.lInt
plot(fit.lInt)
```

**Interaction + log X + Penalty 2**

Here, the penalty for adding a new term during the forward pass is reduced.
This can increase the complexity of the model
but it can also lead to overfitting.

```{r}
fit.lIntP2 <- earth(lX, lY, nk = 200, degree = 2, penalty = 2)
fit.lIntP2
plot(fit.lIntP2)
```

**Interaction + log X + no Outliers**

As indicated the the previous plots there are 3 outlier values (31, 463, 633).
They all actually look quite separated from the rest.
It might be a good idea to exclude them during training.

```{r}
lXr <- lX[-c(31, 463, 633), ]
lYr <- lY[-c(31, 463, 633)]
fit.lIntr <- earth(lXr, lYr, nk = 200, degree = 2)
fit.lIntr
plot(fit.lIntr)
```

**Interaction + log X + Penalty 2 + no Outliers**

These outliers might have also influenced the model with reduced penalty.
Therefore, this model is trained again without these outliers.

```{r}
fit.lIntP2r <- earth(lXr, lYr, nk = 200, degree = 2, penalty = 2)
fit.lIntP2r
plot(fit.lIntP2r)
```


## Error Estimation

Eventually, the error of the learning procedure has to be estimated.
This will be done using cross validation.
With error the root-mean-squared-error (RMSE) of the log of the target variable
is defined by the Kaggle competition guidelines.

```{r}
L <- function(lY, lYh) sqrt(sum((lY - lYh)^2) / length(lY))
```

Inside the cross validation a prediction function will
train a MARS model on one part of the training set 
and then predict the other.
The RMSE for the prediction will be returned.

```{r}
predMARS <- function(trainX, trainY, testX, testY, ...) {
  fit <- earth(trainX, trainY, nk = 200, ...)
  pred <- predict(fit, newdata = testX)
  return(L(testY, pred[, 1]))
}
```

Splitting of the data and execution of the prediction function is performed
by `crossval`.
For each model it will repeat a 5-fold cross validation 5 times.
The resulting mean RMSE and its standard error are stored in a list.

```{r}
CV <- list()
CV$Add <- crossval(predMARS, X, lY, verbose = FALSE, K = 5, B = 5)
#CV$Addr <- crossval(predMARS, Xr, lYr, verbose = FALSE, K = 5, B = 5)
CV$lAdd <- crossval(predMARS, lX, lY, verbose = FALSE, K = 5, B = 5)
CV$lAddr <- crossval(predMARS, lXr, lYr, verbose = FALSE, K = 5, B = 5)
CV$Int <- crossval(predMARS, X, lY, degree = 2, verbose = FALSE, K = 5, B = 5)
CV$lInt <- crossval(predMARS, lX, lY, degree = 2, verbose = FALSE, K = 5, B = 5)
CV$lIntP2 <- crossval(predMARS, lX, lY, degree = 2, penalty = 2, 
                      verbose = FALSE, K = 5, B = 5)
CV$lIntr <- crossval(predMARS, lXr, lYr, degree = 2, 
                     verbose = FALSE, K = 5, B = 5)
CV$lIntP2r <- crossval(predMARS, lXr, lYr, degree = 2, penalty = 2, 
                      verbose = FALSE, K = 5, B = 5)
```

Estimated RMSEs for all models are shown below.

```{r}
l <- lapply(CV, function(i) data.frame(RMSE = i$stat, se = i$stat.se))
res <- do.call(rbind, l)
res$Model <- rownames(res)
ggplot(res, aes(x = Model, y = RMSE)) +
  geom_pointrange(aes(ymax = RMSE + se, ymin = RMSE - se), color = "blue") +
  coord_flip() +
  ggtitle("CV-Estimated RMSE with Standard Error") +
  theme_classic()
```






## Final Model

```{r}
summary(fit.Add, decomp = "none")

evimp(fit.Add)[1:10, ]

plotmo(fit.Add)



```




















***




# Modeling

```{r}
?earth



```

